{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06fdeac",
   "metadata": {},
   "source": [
    "# Overfit Artificial Neural Network\n",
    "\n",
    "## **Emmanuel Ikpesu**\n",
    "#### **Hildebrand Department of Petroleum and Geosystems Engineering, Cockrell School of Engineering**\n",
    "##### [LinkedIn](https://www.linkedin.com/in/emmanuel-ikpesu-393708132/) | [Twitter](https://twitter.com/thedatahokage) | [GitHub](https://github.com/uchiharon) \n",
    "\n",
    "### Subsurface Machine Learning Course, The University of Texas at Austin\n",
    "#### Hildebrand Department of Petroleum and Geosystems Engineering, Cockrell School of Engineering\n",
    "#### Department of Geological Sciences, Jackson School of Geosciences\n",
    "\n",
    "_____________________\n",
    "\n",
    "Workflow supervision and review by:\n",
    "\n",
    "#### Instructor: Prof. Michael Pyrcz, Ph.D., P.Eng., Associate Professor, The Univeristy of Texas at Austin\n",
    "[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [Applied Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
    "\n",
    "#### Course TA: Elnara Rustamzade, Graduate Student, The University of Texas at Austin\n",
    "##### [LinkedIn](https://www.linkedin.com/in/elnara-rustamzade/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d70c8",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "#### Gap:\n",
    "Artificial Neural Networks (ANNs) are powerful tools for modeling nonlinear relationships in data, but they are also prone to overfitting‚Äîwhere the model performs well on training data but poorly on unseen data. This lab investigates the sensitivity of ANNs to architectural and training hyperparameters, and seeks to answer: What factors contribute most to overfitting in neural networks, and how can they be controlled?\n",
    "#### Approach:\n",
    "An interactive ANN model was implemented to simulate predictive regression tasks with synthetic noisy data. Various hyperparameters‚Äîsuch as number of epochs, dropout rate, number of hidden layers and nodes, learning rate, and batch size‚Äîwere explored. Visualizations were used to demonstrate model performance under different settings, particularly observing overfitting patterns between training and testing results.\n",
    "#### Lessons Learned:\n",
    "-   Epoch count is a major contributor to overfitting‚Äîhigher values often lead to memorization of training data.\n",
    "-   Dropout helps mitigate overfitting by randomly disabling nodes, though it may slightly increase prediction error.\n",
    "-   Constant-sized hidden layers promote overfitting compared to gradually reduced architectures.\n",
    "-   Learning rate plays a delicate role: if too high, the model may diverge or overfit quickly; if too low, it may underfit or train inefficiently.\n",
    "-   Model flexibility must be balanced with proper regularization to achieve good generalization.\n",
    "#### Recommendations:\n",
    "-   Apply early stopping or use lower epoch counts to prevent unnecessary training past the optimal point.\n",
    "-   Introduce dropout layers where appropriate to promote model robustness.\n",
    "-   Consider decreasing the number of hidden nodes in deeper layers to reduce model complexity.\n",
    "-   Tune learning rate carefully, potentially using learning rate schedules or search strategies to find the optimal value.\n",
    "-   Use visualization and validation performance as feedback loops to guide model adjustments iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e0e4e1",
   "metadata": {},
   "source": [
    "## Steps for Building My ANN Lab (OUTLINE) üìò  <a id=\"outline\"></a>\n",
    "[ANN Overview](#ann-overview)\n",
    "\n",
    "[Things to Try](#things-to-try-)\n",
    "\n",
    "[Import Packages](#import-packages)\n",
    "\n",
    "[Lab Functions](#lab-functions)\n",
    "- Data Generator\n",
    "- Model Architecture Designer\n",
    "\n",
    "[Overfitting Lab Development](#overfitting-lab-development)\n",
    "- Define number of realizations\n",
    "- Run Sensitivity Analysis on Each Hyperparameter While Keeping Others as Inputted\n",
    "- Build Desired Model\n",
    "- Visualize Final Result\n",
    "\n",
    "[Overfitting Lab Demo](#overfitting-lab-interactive-demonstration)\n",
    "\n",
    "[Observations \\ Conclusion](#observations-)\n",
    "\n",
    "[About Author](#about-the-author)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3519141",
   "metadata": {},
   "source": [
    "## Things to Try\n",
    "[üîù Back to Outline](#outline)\n",
    "-   Vary the Number of Hidden Layers and Neurons\n",
    "    - What happens to training vs. validation error as you increase model capacity?\n",
    "- Adjust Dropout Rate\n",
    "    - Try dropout values between 0 and 0.5. How does regularization affect overfitting?\n",
    "- Experiment with Activation Functions\n",
    "    - Use ReLU, tanh, and sigmoid for the hidden layers. Which one results in better generalization?\n",
    "- Modify Batch Size\n",
    "    - Try small (16) vs. large (128) batches. Does a noisier gradient help regularize the model?\n",
    "- Compare Optimizers\n",
    "    - Use Adam, SGD, and RMSprop. Which one converges faster or helps avoid overfitting?\n",
    "- Vary the Epoch\n",
    "    -  How does it make your model perform/overfit?\n",
    "\n",
    "üöÄ **Pro-Tip: Use Sensitivity Plot for Better Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1b970",
   "metadata": {},
   "source": [
    "## ANN Overview \n",
    "[üîù Back to Outline](#outline)\n",
    "\n",
    "An ANN (Artificial Neural Network) is a machine learning model inspired by how the human brain works. It‚Äôs a type of algorithm that learns from data by mimicking how neurons in the brain activate and pass signals to each other. It is a collection of simple interconnected algorithms that process information in response to external input.\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/uchiharon/ANN-Overfitting-Lab/blob/main/Images/ANN.png?raw=true\" width=\"850\" height=\"300\" />\n",
    "\n",
    "</p>\n",
    "An ANN is made up of layers:\n",
    "\n",
    "- Input layer ‚Äì where data enters the model\n",
    "- Hidden layers ‚Äì where the network processes the data through neurons (nodes)\n",
    "- Output layer ‚Äì where predictions are made\n",
    "\n",
    "üîß What Is It Used For?\n",
    "- Image recognition (like classifying cats vs. dogs)\n",
    "- Natural language processing (like sentiment analysis)\n",
    "- Forecasting and prediction\n",
    "- Game AI and robotics\n",
    "- Finance, medical diagnosis, and more\n",
    "\n",
    "_____________________\n",
    "### ANN Key Components Include:\n",
    "#### Perceptrons (Nodes)\n",
    "This are the building blockers of neural networks. It‚Äôs a single-layer neural network that makes decisions by weighing input features. It's is made up of a **Linear Model** and an **Activation Functions** introduce non-linearity into a neural network, enabling it to learn complex patterns.\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/uchiharon/ANN-Overfitting-Lab/blob/main/Images/perceptron-in-machine-learning2.png?raw=true\" width=\"550\" height=\"300\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i x_i + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( x_i \\) are the input features\n",
    "- \\( w_i \\) are the corresponding weights\n",
    "- \\( b \\) is the bias term\n",
    "\n",
    "The output of the perceptron is:\n",
    "\n",
    "$$\n",
    "y = \\phi(z)\n",
    "$$\n",
    "\n",
    "where $\\phi$ is an activation function, such as:\n",
    "\n",
    "- ReLU: $ \\phi(z) = \\max(0, z)$\n",
    "- Sigmoid: $\\phi(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "To learn more about activation function, please check out the [TensorFlow documentation on activation functions](https://www.tensorflow.org/api_docs/python/tf/keras/activations).\n",
    "\n",
    "#### Loss Function üìâ\n",
    "A loss function quantifies the error between the predicted output of the neural network and the actual target value. During training, the network adjusts its weights to minimize this loss.\n",
    "\n",
    "üîß General Idea:\n",
    "Let:\n",
    "\n",
    "$\\hat{y}_i$: predicted output\n",
    "$y$: true (target) value\n",
    "\n",
    "The loss $\\mathcal{L}$ is a function that measures the discrepancy between $\\hat{y}_i$ and $y$\n",
    "\n",
    "Common Loss Functions Used in ANN:\n",
    "1. **Mean Squared Error (MSE)** ‚Äî for Regression Tasks\n",
    "    - Penalizes large errors more than small ones.\n",
    "    - Smooth and differentiable.\n",
    "$$\n",
    "\\mathcal{L}_{\\text{MSE}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "2. **Mean Absolute Error (MAE)** ‚Äî for Regression Tasks\n",
    "    - Less sensitive to outliers than MSE\n",
    "$$\n",
    "\\mathcal{L}_{\\text{MAE}} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "3. **Binary Cross-Entropy** ‚Äî for Binary Classification\n",
    "    - Best used when outputs are probabilities (e.g. from a sigmoid activation).\n",
    "$$\n",
    "\\mathcal{L}_{\\text{BCE}} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "4. **Categorical Cross-Entropy** ‚Äî for Multi-Class Classification\n",
    "    - $C$: number of classes.\n",
    "    - Typically used with **softmax** output.\n",
    "$$\n",
    "\\mathcal{L}_{\\text{CCE}} = - \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n",
    "$$\n",
    "\n",
    "To learn more about loss function, please check out the [TensorFlow documentation on loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses).\n",
    "\n",
    "#### Optimizers üîÅ\n",
    "An **optimizer** in an Artificial Neural Network (ANN) is the algorithm that **updates the model‚Äôs weights** during training to **minimize the loss function**. Its goal is to make the model learn by improving predictions step by step.\n",
    "\n",
    "At every training step:\n",
    "1. The model makes a prediction.\n",
    "2. The loss function measures how far the prediction is from the actual value.\n",
    "3. The optimizer adjusts the weights and biases to reduce this loss.\n",
    "\n",
    "It uses gradients (from backpropagation) to decide how much and in which direction to change each parameter.\n",
    "\n",
    "Common Optimizers Used in ANN:\n",
    "1. **Gradient Descent**\n",
    "    \n",
    "    Basic weight update rule:\n",
    "    $$\n",
    "    \\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} \\mathcal{L}(\\theta)\n",
    "    $$\n",
    "\n",
    "    - $\\theta$: model parameters (weights)\n",
    "    - $\\eta$: learning rate\n",
    "    - $\\nabla_{\\theta} \\mathcal{L}$: gradient of the loss function with respect to \n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "    Similar to gradient descent but uses one (or a few) training examples:\n",
    "    $$\n",
    "    \\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} \\mathcal{L}(\\theta; x_i, y_i)\n",
    "    $$\n",
    "\n",
    "3. **Momentum**\n",
    "    Adds a velocity term $ùë£$ to smooth updates:\n",
    "    $$\n",
    "    v = \\beta v + \\eta \\cdot \\nabla_{\\theta} \\mathcal{L}(\\theta) \\\\\n",
    "    \\theta = \\theta - v\n",
    "    $$\n",
    "\n",
    "    - $\\beta$: momentum coefficient (e.g., 0.9)\n",
    "\n",
    "To learn more about optimizers, please check out the [TensorFlow documentation on optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).\n",
    "\n",
    "#### Training Cycles\n",
    "A training cycle involves:\n",
    "- Presenting a batch of data to the model.\n",
    "- Performing a forward pass to generate predictions.\n",
    "- Calculating the prediction error.\n",
    "- Applying backpropagation to update the model‚Äôs parameters and reduce the overall error.\n",
    "- This process is repeated iteratively to improve model accuracy.\n",
    "\n",
    "#### Batch\n",
    "A batch is a subset of the training data used in a single training cycle.\n",
    "\n",
    "There‚Äôs a trade-off in batch size:\n",
    "- Larger batches ‚Üí More accurate error estimates, but increased computational cost per cycle.\n",
    "- Smaller batches ‚Üí Faster computation and potentially better generalization due to noisier updates.\n",
    "\n",
    "#### Epoch\n",
    "An epoch consists of one full pass through the entire training dataset, broken into batches. Multiple epochs are typically required for a model to converge.\n",
    "\n",
    "#### Connections\n",
    "Connections are the weighted links between nodes in adjacent layers of a neural network.\n",
    "In a fully connected (dense) network:\n",
    "- Each node in one layer connects to every node in the next layer.\n",
    "- Each connection carries a weight that is adjusted during training to learn the optimal mapping from inputs to outputs.\n",
    "_____________________\n",
    "### Limitations of Neural Network Estimation ‚ö†Ô∏è\n",
    "While artificial neural networks (ANNs) can be powerful tools for estimation from sparsely sampled data across depth, there are several important limitations to consider in this context:\n",
    "\n",
    "- Does not honor well data  ‚ùå‚Äì Predictions may not strictly follow known measurements at well locations.\n",
    "- Lacks spatial correlation üåê ‚Äì ANNs typically do not account for spatial continuity between data points.\n",
    "- Low interpretability üß† ‚Äì The model behaves like a black box, making it hard to understand or explain.\n",
    "- Requires large training data üìà ‚Äì Neural networks generally perform poorly with limited data.\n",
    "- High complexity and variance ‚öôÔ∏è ‚Äì Complex models can overfit and exhibit unstable performance across different datasets.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd6f6e",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "[üîù Back to Outline](#outline)\n",
    ">  Below are the packages utilized for building this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1642afc-a89c-404b-8c3b-fd7b3b82d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np                                      # arrays and matrix math\n",
    "import matplotlib.pyplot as plt                         # plotting\n",
    "from sklearn.model_selection import train_test_split    # train and test split\n",
    "from sklearn.metrics import mean_squared_error          # model error calculation\n",
    "from sklearn.preprocessing import StandardScaler        # standardize data\n",
    "import scipy                                            # kernel density estimator for PDF plot\n",
    "\n",
    "\n",
    "# build deep learning models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta\n",
    "\n",
    "\n",
    "from ipywidgets import interactive                      # widgets and interactivity\n",
    "from ipywidgets import widgets                            \n",
    "from ipywidgets import Layout\n",
    "from ipywidgets import Label\n",
    "from ipywidgets import VBox, HBox\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')                       # supress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ae826",
   "metadata": {},
   "source": [
    "## Lab Functions\n",
    "[üîù Back to Outline](#outline)\n",
    "> The following functions will be used in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24bda767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA GENERATION FUNCTION\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------\n",
    "# This function generates a synthetic dataset based on a polynomial function with added noise.\n",
    "\n",
    "# Source: Dr. Michael Pyrcz (GeostatsGuy)\n",
    "# https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Overfit.ipynb\n",
    "# Adapted from: \"Interactive Overfit\" Notebook\n",
    "\n",
    "def data_generator(n: int, std: float, seed: int):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset based on a polynomial function with added noise.\n",
    "    \n",
    "    Parameters:\n",
    "    - n (int): Number of data points.\n",
    "    - split (float): Proportion of the dataset to include in the test split (e.g., 0.2 for 20% test data).\n",
    "    - std (float): Standard deviation of the Gaussian noise added to the data.\n",
    "    - seed (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - X (array): Predictor feature values.\n",
    "    - y (array): Target values (response variable).\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)  # Seed the random number generator for reproducibility\n",
    "\n",
    "    # Generate dataset\n",
    "    X_seq = np.linspace(0, 20, 100)  # Sequence for plotting\n",
    "    X = np.random.rand(n) * 20  # Generate random X values within the range [0, 20]\n",
    "    \n",
    "    # Create polynomial target values (quadratic function in this case)\n",
    "    y = X ** 2 + 50.0  \n",
    "    \n",
    "    # Add Gaussian noise to the target variable\n",
    "    y += np.random.normal(loc=0.0, scale=std, size=n)\n",
    "\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaled = X_scaler.fit_transform(X.reshape(-1, 1)).flatten()\n",
    "\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "    return X_scaled, y_scaled, X_scaler, y_scaler\n",
    "\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.2, seed=42, realization=0):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X (array-like): Feature matrix.\n",
    "    - y (array-like): Target variable.\n",
    "    - test_size (float): Proportion of the dataset to include in the test split.\n",
    "    - seed (int): Random seed for reproducibility.\n",
    "    - realization (int): Value added to the seed for multiple realizations.\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test: Split training and testing datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=seed + realization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc183f21-a7ce-42ac-ad36-35d778932ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------MODEL BUILDING FUNCTIONS-----------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Code generated with the assistance of ChatGPT (OpenAI, 2025)\n",
    "# and modified for clarity and functionality.\n",
    "# The following functions are used to build a neural network model using Keras.\n",
    "\n",
    "def input_layer(nodes: int, activation_function: str):\n",
    "    \"\"\"\n",
    "    Creates an input layer for a neural network.\n",
    "\n",
    "    Parameters:\n",
    "    nodes (int): Number of neurons in the input layer.\n",
    "    activation_function (str): Activation function to be used in the input layer.\n",
    "\n",
    "    Returns:\n",
    "    Dense: A Keras Dense layer with the specified number of nodes and activation function.\n",
    "    \"\"\"\n",
    "    return Dense(nodes, activation=activation_function, input_shape=(1,))\n",
    "\n",
    "\n",
    "def hidden_layer(nodes: int, activation_function: str):\n",
    "    \"\"\"\n",
    "    Creates a hidden layer for a neural network.\n",
    "\n",
    "    Parameters:\n",
    "    nodes (int): The number of neurons in the hidden layer.\n",
    "    activation_function (str): The activation function to be applied to the layer.\n",
    "\n",
    "    Returns:\n",
    "    Dense: A Keras Dense layer with the specified number of neurons and activation function.\n",
    "\n",
    "    Example:\n",
    "    >>> layer = hidden_layer(10, 'relu')\n",
    "    >>> print(layer)\n",
    "    <keras.src.layers.core.dense.Dense object at 0x...>\n",
    "    \"\"\"\n",
    "    return Dense(nodes, activation=activation_function)\n",
    "\n",
    "\n",
    "def output_layer():\n",
    "    \"\"\"\n",
    "    Creates an output layer for a neural network.\n",
    "\n",
    "    Returns:\n",
    "    Dense: A Keras Dense layer with one neuron and a linear activation function.\n",
    "    \"\"\"\n",
    "    return Dense(1, activation='linear')\n",
    "\n",
    "\n",
    "def create_hidden_layers(model, num_layers: int, start_nodes: int, reduce_nodes: bool, activation_function: str, dropout_rate: float):\n",
    "    \"\"\"\n",
    "    Adds hidden layers to a model with the option to either reduce or keep constant the number of neurons.\n",
    "\n",
    "    Parameters:\n",
    "    - model (Sequential): The Keras Sequential model to which the layers will be added.\n",
    "    - num_layers (int): Number of hidden layers to add.\n",
    "    - start_nodes (int): The number of neurons in the first hidden layer.\n",
    "    - reduce_nodes (bool): If True, the number of neurons will decrease in each layer.\n",
    "    - activation_function (str): The activation function to use for the hidden layers.\n",
    "\n",
    "    Returns:\n",
    "    - model (Sequential): The updated Keras Sequential model with added hidden layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        if reduce_nodes:\n",
    "            # Decrease the number of neurons by half in each layer\n",
    "            nodes = max(1, int(start_nodes / (2 ** i)))  # Avoid going below 1 neuron\n",
    "        else:\n",
    "            # Keep the number of neurons constant\n",
    "            nodes = start_nodes\n",
    "\n",
    "        model.add(hidden_layer(nodes, activation_function))\n",
    "        model.add(Dropout(dropout_rate))  # Add dropout layer\n",
    "        # Add a dropout layer after each hidden layer\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(input_nodes: int, input_activation: str,\n",
    "                num_hidden_layers: int, hidden_nodes: int, reduce_nodes: bool = True, hidden_activation: str = 'relu', dropout_rate: float = 0.0,\n",
    "                optimizer: str = 'adam', learning_rate: float = 0.01):\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds a customizable neural network model using the Sequential API.\n",
    "\n",
    "    This function creates a neural network model with a specified number of hidden layers, \n",
    "    customizable hidden node sizes (with an option for reducing nodes), activation functions, \n",
    "    and dropout regularization.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_nodes : int\n",
    "        The number of nodes in the input layer.\n",
    "    \n",
    "    input_activation : str\n",
    "        The activation function for the input layer (e.g., 'ReLU', 'sigmoid', 'tanh').\n",
    "    \n",
    "    num_hidden_layers : int\n",
    "        The number of hidden layers to be added to the model.\n",
    "    \n",
    "    hidden_nodes : int\n",
    "        The number of nodes in the first hidden layer. The number of nodes in subsequent \n",
    "        layers will decrease if `reduce_nodes` is set to True.\n",
    "    \n",
    "    reduce_nodes : bool, default=True\n",
    "        Whether to reduce the number of nodes in each subsequent hidden layer. If False, \n",
    "        all hidden layers will have the same number of nodes as the first hidden layer.\n",
    "    \n",
    "    hidden_activation : str, default='ReLU'\n",
    "        The activation function for all hidden layers (e.g., 'ReLU', 'sigmoid', 'tanh').\n",
    "    \n",
    "    dropout_rate : float, default=0.0\n",
    "        The dropout rate applied to the hidden layers to prevent overfitting. A value between 0 and 1.\n",
    "\n",
    "    optimizer : str, default='adam'\n",
    "        The optimizer to use for training the model. Options include 'adam', 'sgd', 'adadelta', and 'rmsprop'.\n",
    "\n",
    "    learning_rate : float, default=0.01\n",
    "        The learning rate for the optimizer. This controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model : keras.Sequential\n",
    "        A compiled Sequential model ready to be trained.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    # Add the input layer\n",
    "    model.add(input_layer(input_nodes, input_activation))\n",
    "    # Add hidden layers\n",
    "    model = create_hidden_layers(model, num_hidden_layers, hidden_nodes, reduce_nodes, hidden_activation, dropout_rate)\n",
    "    # Add the output layer\n",
    "    model.add(output_layer())\n",
    "\n",
    "    # Choose optimizer based on input\n",
    "    if optimizer == 'Adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'Adadelta':\n",
    "        opt = Adadelta(learning_rate=learning_rate)\n",
    "    elif optimizer == 'RMSprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, epochs: int = 10, batch_size: int = 32, val=False):\n",
    "    \"\"\"\n",
    "    Trains the neural network model on the provided training data.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : keras.Sequential\n",
    "        The compiled Keras model to be trained.\n",
    "    \n",
    "    X_train : np.ndarray\n",
    "        The input features for training.\n",
    "    \n",
    "    y_train : np.ndarray\n",
    "        The target variable for training.\n",
    "    \n",
    "    epochs : int, default=10\n",
    "        The number of epochs to train the model.\n",
    "    \n",
    "    batch_size : int, default=32\n",
    "        The size of the batches used in training.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    history : keras.callbacks.History\n",
    "        A History object containing details about the training process.\n",
    "    \"\"\"\n",
    "\n",
    "    if val:\n",
    "        # Split the data into training and validation sets\n",
    "        X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "        history = model.fit(X_train1, y_train1, epochs=epochs, batch_size=batch_size, verbose=0, validation_data=(X_val, y_val))\n",
    "        # Train the model with validation data\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    return model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbcc235",
   "metadata": {},
   "source": [
    "## Overfitting Lab Development\n",
    "[üîù Back to Outline](#outline)\n",
    "> In the below workflow, I created a simple lab to study overfitting in ANN. Below are the follow steps taken to achieve it:\n",
    "- Define sensitizing parameters.\n",
    "- Create interactive widgets\n",
    "- Build the model (The complex and compute resource demanding part of the workflow):\n",
    "    - Defined dictionary to store realizations values\n",
    "    - Loop through each hyperparameter to be sensitized on and store model values\n",
    "    - Summary the value realized from the multiple realization of the sensitized hyperparameter\n",
    "    - Build the desired model\n",
    "    - Evaluate the model\n",
    "- Plot desired results obtained after model training and evaluation\n",
    "- Connect the interactive widgets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c890ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------HYPERPARAMETERS------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------\n",
    " \n",
    "activation_functions_list = ['linear', 'relu', 'sigmoid', 'softmax', 'gelu']\n",
    "reduce_nodes_list = [True, False]\n",
    "node_sizes_list = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "num_layers_list = [1, 2, 3, 4, 5]\n",
    "dropout_rates_list = [0.0, 0.01, 0.03, 0.05, 0.075, 0.1, 0.2, 0.3]\n",
    "epochs_list = [1, 10, 100, 1000]  # 5 epochs from 1 to 5\n",
    "learning_rates_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "batch_sizes_list = [1, 2, 4, 8, 16, 32]\n",
    "optimizers_list = ['SGD', 'Adam', 'Adadelta', 'RMSprop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6809755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------INTERACTIVE WIDGETS----------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------\n",
    "title = widgets.Text(value='                                                                                            Artificial Neural Networked Overfitting Demo, Emmanuel Ikpesu, The University of Texas at Austin',\n",
    "                 layout=Layout(width='1280px', height='30px'))\n",
    "\n",
    "# data parameters\n",
    "title_left = widgets.HTML(value=\"<b>üìä Data Parameters</b>\")\n",
    "n = widgets.IntSlider(min=15, max=80, value=30, step=1, description='Data Size (n)', style={'description_width': 'initial'})\n",
    "split = widgets.FloatSlider(min=0.05, max=0.95, value=0.20, step=0.05, description='Test Split %', style={'description_width': 'initial'})\n",
    "std = widgets.FloatSlider(min=0, max=50, value=0, step=1.0, description='Noise StDev', style={'description_width': 'initial'})\n",
    "\n",
    "\n",
    "\n",
    "# Input layer parameters\n",
    "title_input = widgets.HTML(value=\"<b>üì• Input Layer</b>\")\n",
    "input_activation = widgets.Dropdown(options=activation_functions_list,\n",
    "                                       value='relu', description='Activation')\n",
    "input_nodes = widgets.Dropdown(options=node_sizes_list, value=64, description='Nodes Per Layer')\n",
    "\n",
    "title_hidden = widgets.HTML(value=\"<b>ü§ñ Hidden Layers</b>\")\n",
    "hidden_activation = widgets.Dropdown(options=activation_functions_list,\n",
    "                                       value='relu', description='Activation')\n",
    "reduce_nodes = widgets.ToggleButtons(options=reduce_nodes_list, value=True, description='Reduce Nodes')\n",
    "hidden_nodes = widgets.Dropdown(options=node_sizes_list, value=128, description='Nodes Per Layer')\n",
    "num_hidden_layers = widgets.IntSlider(min=1, max=4, value=2, step=1, description='Num Hidden Layers')\n",
    "dropout_rate = widgets.FloatSlider(min=0.0, max=0.3, value=0.075, step=0.1, description='Dropout Rate')\n",
    "\n",
    "# ANN hyperparameters\n",
    "title_ann = widgets.HTML(value=\"<b>üß† ANN Parameters</b>\")\n",
    "epochs = widgets.Dropdown(options=epochs_list, value=1000, description='Epochs')\n",
    "learning_rate = widgets.Dropdown(options=learning_rates_list, value=0.01, description='Learning Rate')\n",
    "batch_size = widgets.Dropdown(options=batch_sizes_list, value=4, description='Batch Size')\n",
    "optimizer = widgets.Dropdown(options=optimizers_list, value='Adam', description='Optimizer')\n",
    "\n",
    "data_column = widgets.VBox([title_left, n, split, std])\n",
    "input_column = widgets.VBox([title_input, input_activation, input_nodes])\n",
    "hidden_column = widgets.VBox([title_hidden, hidden_activation, hidden_nodes, num_hidden_layers, dropout_rate, reduce_nodes])\n",
    "ann_column = widgets.VBox([title_ann, epochs, learning_rate, batch_size, optimizer])\n",
    "\n",
    "model_details = widgets.Text(value=f'''                                                             The ANN Model Currently have {num_hidden_layers.value} \n",
    "Hidden Layers, Epoch of {epochs.value}, & {learning_rate.value} Learning Rate. For more details, please check the bottons above.''',\n",
    "                            \n",
    "                              \n",
    "                 layout=Layout(width='1280px', height='30px'))\n",
    "\n",
    "\n",
    "\n",
    "# Combine into two-column layout\n",
    "ui = HBox([data_column, input_column, hidden_column, ann_column])\n",
    "ui2 = VBox([title, ui, model_details])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc956de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: Dr. Michael Pyrcz (GeostatsGuy)\n",
    "# https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Overfit.ipynb\n",
    "# Adapted from: \"Interactive Overfit\" Notebook\n",
    "def main(n, split, std, input_nodes, input_activation, num_hidden_layers, hidden_nodes, \\\n",
    "         reduce_nodes, hidden_activation, dropout_rate, optimizer, learning_rate, batch_size, epochs):\n",
    "\n",
    "\n",
    "    # Number of realizations for the model sensitivity analysis\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------------\n",
    "    nreal  = 4\n",
    "    seed = 42\n",
    "    np.random.seed(seed)                                   # set seed for reproducibility\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------HYPERPARAMETERS------------------------------------------------------------\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    activation_functions_list = ['linear', 'relu', 'sigmoid', 'softmax', 'gelu']\n",
    "    reduce_nodes_list = [True, False]\n",
    "    node_sizes_list = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    num_layers_list = [1, 2, 3, 4, 5]\n",
    "    dropout_rates_list = [0.0, 0.01, 0.03, 0.05, 0.075, 0.1, 0.2, 0.3]\n",
    "    epochs_list = [1, 10, 100, 1000]\n",
    "    learning_rates_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "    batch_sizes_list = [1, 2, 4, 8, 16, 32]\n",
    "    optimizers_list = ['SGD', 'Adam', 'Adadelta', 'RMSprop']\n",
    "\n",
    "\n",
    "    # Generate data\n",
    "    X_scaled, y_scaled, X_scaler, y_scaler = data_generator(n, std, seed)\n",
    "\n",
    "\n",
    "    # Dictionary to store MSE results\n",
    "    mse_results = {\n",
    "        'hidden_node_sizes': \n",
    "            {'train': np.zeros([len(node_sizes_list), nreal])\n",
    "            ,'test': np.zeros([len(node_sizes_list), nreal])},\n",
    "        'num_layers': \n",
    "            {'train': np.zeros([len(num_layers_list), nreal])\n",
    "            ,'test': np.zeros([len(num_layers_list), nreal])},\n",
    "        'dropout_rates': \n",
    "            {'train': np.zeros([len(dropout_rates_list), nreal])\n",
    "            ,'test': np.zeros([len(dropout_rates_list), nreal])},\n",
    "        'epochs_list': \n",
    "            {'train': np.zeros([len(epochs_list), nreal])\n",
    "            ,'test': np.zeros([len(epochs_list), nreal])},\n",
    "        'learning_rates': \n",
    "            {'train': np.zeros([len(learning_rates_list), nreal])\n",
    "            ,'test': np.zeros([len(learning_rates_list), nreal])},\n",
    "        'batch_sizes': \n",
    "            {'train': np.zeros([len(batch_sizes_list), nreal])\n",
    "            ,'test': np.zeros([len(batch_sizes_list), nreal])}   \n",
    "    }\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    #--------------------------------------------------------------Run Model Sensitivity----------------------------------------------------------\n",
    "    for j in range(nreal):\n",
    "        # Split data for each realization\n",
    "        X_train, X_test, y_train, y_test = split_data(X_scaled, y_scaled, split, seed, j)\n",
    "\n",
    "        # Test different Hidden node sizes\n",
    "        for i, nodes in enumerate(node_sizes_list):\n",
    "            model = build_model(input_nodes, input_activation,\n",
    "                    num_hidden_layers, nodes, reduce_nodes, hidden_activation, dropout_rate,\n",
    "                    optimizer, learning_rate)\n",
    "            model, history = train_model(model, X_train, y_train, epochs, batch_size)\n",
    "            y_train_pred = model.predict(X_train, verbose=0)\n",
    "            y_test_pred = model.predict(X_test, verbose=0)\n",
    "            mse_results['hidden_node_sizes']['train'][i, j] = mean_squared_error(y_train, y_train_pred)\n",
    "            mse_results['hidden_node_sizes']['test'][i, j] = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "        # Test different number of layers\n",
    "        for i, layers in enumerate(num_layers_list):\n",
    "            model = build_model(input_nodes, input_activation,\n",
    "                    layers, hidden_nodes, reduce_nodes, hidden_activation, dropout_rate,\n",
    "                    optimizer, learning_rate)\n",
    "            model, history = train_model(model, X_train, y_train, epochs, batch_size)\n",
    "            y_train_pred = model.predict(X_train, verbose=0)\n",
    "            y_test_pred = model.predict(X_test, verbose=0)\n",
    "            mse_results['num_layers']['train'][i, j] = mean_squared_error(y_train, y_train_pred)\n",
    "            mse_results['num_layers']['test'][i, j] = mean_squared_error(y_test, y_test_pred)\n",
    "            \n",
    "\n",
    "        # Test different dropout rates\n",
    "        for i, dropout in enumerate(dropout_rates_list):\n",
    "            model = build_model(input_nodes, input_activation,\n",
    "                    num_hidden_layers, hidden_nodes, reduce_nodes, hidden_activation, dropout,\n",
    "                    optimizer, learning_rate)\n",
    "            model, history = train_model(model, X_train, y_train, epochs, batch_size)\n",
    "            y_train_pred = model.predict(X_train, verbose=0)\n",
    "            y_test_pred = model.predict(X_test, verbose=0)\n",
    "            mse_results['dropout_rates']['train'][i, j] = mean_squared_error(y_train, y_train_pred)\n",
    "            mse_results['dropout_rates']['test'][i, j] = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "        # Test different number of epochs\n",
    "        for i, epoch_value in enumerate(epochs_list):\n",
    "            model = build_model(input_nodes, input_activation,\n",
    "                    num_hidden_layers, hidden_nodes, reduce_nodes, hidden_activation, dropout_rate,\n",
    "                    optimizer, learning_rate)\n",
    "            model, history = train_model(model, X_train, y_train, epoch_value, batch_size)\n",
    "            y_train_pred = model.predict(X_train, verbose=0)\n",
    "            y_test_pred = model.predict(X_test, verbose=0)\n",
    "            mse_results['epochs_list']['train'][i, j] = mean_squared_error(y_train, y_train_pred)\n",
    "            mse_results['epochs_list']['test'][i, j] = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "        # Test different learning rates\n",
    "        for i, lr in enumerate(learning_rates_list):\n",
    "            model = build_model(input_nodes, input_activation,\n",
    "                    num_hidden_layers, hidden_nodes, reduce_nodes, hidden_activation, dropout_rate,\n",
    "                    optimizer, lr)\n",
    "            model, history = train_model(model, X_train, y_train, epochs, batch_size)\n",
    "            y_train_pred = model.predict(X_train, verbose=0)\n",
    "            y_test_pred = model.predict(X_test, verbose=0)\n",
    "            mse_results['learning_rates']['train'][i, j] = mean_squared_error(y_train, y_train_pred)\n",
    "            mse_results['learning_rates']['test'][i, j] = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "        # Test different batch sizes\n",
    "        for i, batch in enumerate(batch_sizes_list):\n",
    "            model = build_model(input_nodes, input_activation,\n",
    "                    num_hidden_layers, hidden_nodes, reduce_nodes, hidden_activation, dropout_rate,\n",
    "                    optimizer, learning_rate)\n",
    "            model, history = train_model(model, X_train, y_train, epochs, batch)\n",
    "            y_train_pred = model.predict(X_train, verbose=0)\n",
    "            y_test_pred = model.predict(X_test, verbose=0)\n",
    "            mse_results['batch_sizes']['train'][i, j] = mean_squared_error(y_train, y_train_pred)\n",
    "            mse_results['batch_sizes']['test'][i, j] = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    mse_summary = {}\n",
    "\n",
    "    for key, value in mse_results.items():\n",
    "        mse_summary[key] = {}\n",
    "        for subset in ['train', 'test']:\n",
    "            mse_matrix = value[subset]\n",
    "            mse_summary[key][subset] = {\n",
    "                'mean': np.mean(mse_matrix, axis=1).flatten(),\n",
    "                'high': np.percentile(mse_matrix, 90, axis=1),\n",
    "                'low': np.percentile(mse_matrix, 10, axis=1)\n",
    "            }\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    #------------------------------------------------------Model Building and Training Required Mode----------------------------------------------\n",
    "\n",
    "    # build the one model example to show\n",
    "    X_train, X_test, y_train, y_test = split_data(X_scaled, y_scaled, split, seed, 0)\n",
    "    model = build_model(input_nodes, input_activation,\n",
    "                    num_hidden_layers, hidden_nodes, reduce_nodes, hidden_activation, dropout_rate,\n",
    "                    optimizer, learning_rate)\n",
    "    model, history = train_model(model, X_train, y_train, epochs, batch_size, val=True)\n",
    "\n",
    "    # calculate error\n",
    "    y_train_pred = model.predict(X_train, verbose=0) # predict training data\n",
    "    y_test_pred = model.predict(X_test, verbose=0) # predict test data\n",
    "\n",
    "\n",
    "    error_train = y_train_pred.flatten() - y_train # calculate error on training data\n",
    "    error_test = y_test_pred.flatten() - y_test # calculate error on test data\n",
    "\n",
    "    # calculate model curve\n",
    "    X_seq = np.linspace(-3,3,100)\n",
    "    y_seq = model.predict(X_seq, verbose=0) # predict model curve\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    #---------------------------------------------------------------Results Visualization--------------------------------------------------\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    plt.figure(figsize=(10, 21))\n",
    "    plt.subplot(331)\n",
    "    # Plot model predictions\n",
    "    X_train_true = X_scaler.inverse_transform(X_train.reshape(-1, 1)).flatten()\n",
    "    X_test_true = X_scaler.inverse_transform(X_test.reshape(-1, 1)).flatten()\n",
    "    y_train_true = y_scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_true = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    X_seq_true = X_scaler.inverse_transform(X_seq.reshape(-1, 1)).flatten()\n",
    "    y_seq_true = y_scaler.inverse_transform(y_seq.reshape(-1, 1)).flatten()\n",
    "    plt.plot(X_seq_true, y_seq_true, color=\"black\", linewidth=2, label=\"ANN Prediction\")\n",
    "    # Plot training and test data\n",
    "    plt.scatter(X_train_true, y_train_true, c=\"red\", alpha=0.3, edgecolors=\"black\", label=\"Training Data\")\n",
    "    plt.scatter(X_test_true, y_test_true, c=\"blue\", alpha=0.3, edgecolors=\"black\", label=\"Test Data\")\n",
    "    plt.title(\"Artificial Neural Network Model\", fontsize=11) # Titles and labels\n",
    "    plt.xlabel(\"Porosity (%)\", fontsize=10); plt.ylabel(\"Permeability (mD)\", fontsize=10)\n",
    "    plt.xlim([0, 25]); plt.ylim([0, 500])\n",
    "    plt.grid(True, linestyle='--', alpha=0.5); plt.legend(frameon=True, loc='upper left')\n",
    "\n",
    "    plt.subplot(332)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(\"ANN Training and Validation Loss\", fontsize=11) # Titles and labels\n",
    "    plt.xlabel(\"Epoch\", fontsize=10); plt.ylabel(\"Mean Squared Error\", fontsize=10)\n",
    "    plt.xscale('log'); plt.yscale('log')\n",
    "    plt.grid(which='both',linestyle='--', alpha=0.5); plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(333)\n",
    "    plt.hist(error_train, facecolor='red',alpha=0.2,density=True,edgecolor='black',label='Train')\n",
    "    plt.hist(error_test, facecolor='blue',alpha=0.2,density=True,edgecolor='black',label='Test')\n",
    "    plt.xlabel('Model Error', fontsize=10); plt.ylabel('Frequency', fontsize=10); \n",
    "    plt.title('Training and Testing Error Distribution', fontsize=11)\n",
    "    plt.legend(loc='upper right'); plt.grid(which='both',linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "    plt.subplot(334)\n",
    "    epoch_summary = mse_summary['epochs_list']\n",
    "    train_avg = epoch_summary['train']['mean']; train_high = epoch_summary['train']['high']; train_low = epoch_summary['train']['low']\n",
    "    test_avg = epoch_summary['test']['mean']; test_high = epoch_summary['test']['high']; test_low = epoch_summary['test']['low']\n",
    "    ax = plt.gca()\n",
    "    plt.plot(epochs_list,train_avg,lw=2,label='Train',c='red')\n",
    "    ax.fill_between(epochs_list,train_high,train_low,facecolor='red',alpha=0.05)\n",
    "    plt.plot(epochs_list,test_avg,lw=2,label='Test',c='blue')  \n",
    "    ax.fill_between(epochs_list,test_high,test_low,facecolor='blue',alpha=0.05)\n",
    "    plt.xscale('log'); plt.yscale('log')\n",
    "    plt.xlabel('Complexity - No_of_epochs'); plt.ylabel('Mean Square Error'); plt.title('Training and Testing Error vs. Model Complexity (Epochs)')\n",
    "    combined_error = np.concatenate((train_avg, test_avg), axis=0)\n",
    "    plt.plot([epochs,epochs],[max(combined_error)*1.95,min(combined_error)*0.05],c = 'black',linewidth=3,alpha = 0.8)\n",
    "    plt.legend(loc='upper right'); plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "    plt.subplot(335)\n",
    "    hidden_node_summary = mse_summary['hidden_node_sizes']\n",
    "    train_avg = hidden_node_summary['train']['mean']; train_high = hidden_node_summary['train']['high']; train_low = hidden_node_summary['train']['low']\n",
    "    test_avg = hidden_node_summary['test']['mean']; test_high = hidden_node_summary['test']['high']; test_low = hidden_node_summary['test']['low']\n",
    "    ax = plt.gca()\n",
    "    plt.plot(node_sizes_list,train_avg,lw=2,label='Train',c='red')\n",
    "    ax.fill_between(node_sizes_list,train_high,train_low,facecolor='red',alpha=0.05)\n",
    "    plt.plot(node_sizes_list,test_avg,lw=2,label='Test',c='blue')  \n",
    "    ax.fill_between(node_sizes_list,test_high,test_low,facecolor='blue',alpha=0.05)\n",
    "    plt.xlabel('Complexity - # Starting Hidden Node'); plt.ylabel('Mean Square Error'); plt.title('Training and Testing Error vs. Model Complexity (# Hidden-Nodes)')\n",
    "    combined_error = np.concatenate((train_avg, test_avg), axis=0)\n",
    "    plt.plot([hidden_nodes,hidden_nodes],[max(combined_error)*1.95,min(combined_error)*0.05],c = 'black',linewidth=3,alpha = 0.8)\n",
    "    plt.legend(loc='upper right'); plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "    plt.subplot(336)\n",
    "    num_layers_summary = mse_summary['num_layers']\n",
    "    train_avg = num_layers_summary['train']['mean']; train_high = num_layers_summary['train']['high']; train_low = num_layers_summary['train']['low']\n",
    "    test_avg = num_layers_summary['test']['mean']; test_high = num_layers_summary['test']['high']; test_low = num_layers_summary['test']['low']\n",
    "    ax = plt.gca()\n",
    "    plt.plot(num_layers_list,train_avg,lw=2,label='Train',c='red')\n",
    "    ax.fill_between(num_layers_list,train_high,train_low,facecolor='red',alpha=0.05)\n",
    "    plt.plot(num_layers_list,test_avg,lw=2,label='Test',c='blue')  \n",
    "    ax.fill_between(num_layers_list,test_high,test_low,facecolor='blue',alpha=0.05)\n",
    "    plt.xlabel('Complexity - # Hidden Layers'); plt.ylabel('Mean Square Error'); plt.title('Training and Testing Error vs. Model Complexity (# Hidden-Layers)')\n",
    "    combined_error = np.concatenate((train_avg, test_avg), axis=0)\n",
    "    plt.plot([num_hidden_layers,num_hidden_layers],[max(combined_error)*1.95,min(combined_error)*0.05],c = 'black',linewidth=3,alpha = 0.8)\n",
    "    plt.legend(loc='upper right'); plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "    plt.subplot(337)\n",
    "    dropout_rates_summary = mse_summary['dropout_rates']\n",
    "    train_avg = dropout_rates_summary['train']['mean']; train_high = dropout_rates_summary['train']['high']; train_low = dropout_rates_summary['train']['low']\n",
    "    test_avg = dropout_rates_summary['test']['mean']; test_high = dropout_rates_summary['test']['high']; test_low = dropout_rates_summary['test']['low']\n",
    "    ax = plt.gca()\n",
    "    plt.plot(dropout_rates_list,train_avg,lw=2,label='Train',c='red')\n",
    "    ax.fill_between(dropout_rates_list,train_high,train_low,facecolor='red',alpha=0.05)\n",
    "    plt.plot(dropout_rates_list,test_avg,lw=2,label='Test',c='blue')  \n",
    "    ax.fill_between(dropout_rates_list,test_high,test_low,facecolor='blue',alpha=0.05)\n",
    "    plt.xlabel('Complexity - Dropout Rate'); plt.ylabel('Mean Square Error'); plt.title('Training and Testing Error vs. Model Complexity (Dropout Rate)')\n",
    "    combined_error = np.concatenate((train_avg, test_avg), axis=0)\n",
    "    plt.plot([dropout_rate,dropout_rate],[max(combined_error)*1.95,min(combined_error)*0.05],c = 'black',linewidth=3,alpha = 0.8)\n",
    "    plt.legend(loc='upper right'); plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.subplot(338)\n",
    "    learning_rates_summary = mse_summary['learning_rates']\n",
    "    train_avg = learning_rates_summary['train']['mean']; train_high = learning_rates_summary['train']['high']; train_low = learning_rates_summary['train']['low']\n",
    "    test_avg = learning_rates_summary['test']['mean']; test_high = learning_rates_summary['test']['high']; test_low = learning_rates_summary['test']['low']\n",
    "    ax = plt.gca()\n",
    "    plt.plot(learning_rates_list,train_avg,lw=2,label='Train',c='red')\n",
    "    ax.fill_between(learning_rates_list,train_high,train_low,facecolor='red',alpha=0.05)\n",
    "    plt.plot(learning_rates_list,test_avg,lw=2,label='Test',c='blue')  \n",
    "    ax.fill_between(learning_rates_list,test_high,test_low,facecolor='blue',alpha=0.05)\n",
    "    plt.xscale('log'); plt.yscale('log')\n",
    "    plt.xlabel('Complexity - Learning Rate'); plt.ylabel('Mean Square Error'); plt.title('Training and Testing Error vs. Model Complexity (Learning Rate)')\n",
    "    combined_error = np.concatenate((train_avg, test_avg), axis=0)\n",
    "    plt.plot([learning_rate,learning_rate],[max(combined_error)*1.95,min(combined_error)*0.05],c = 'black',linewidth=3,alpha = 0.8)\n",
    "    plt.legend(loc='upper right'); plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.subplot(339)\n",
    "    batch_sizes_summary = mse_summary['batch_sizes']\n",
    "    train_avg = batch_sizes_summary['train']['mean']; train_high = batch_sizes_summary['train']['high']; train_low = batch_sizes_summary['train']['low']\n",
    "    test_avg = batch_sizes_summary['test']['mean']; test_high = batch_sizes_summary['test']['high']; test_low = batch_sizes_summary['test']['low']\n",
    "    ax = plt.gca()\n",
    "    plt.plot(batch_sizes_list,train_avg,lw=2,label='Train',c='red')\n",
    "    ax.fill_between(batch_sizes_list,train_high,train_low,facecolor='red',alpha=0.05)\n",
    "    plt.plot(batch_sizes_list,test_avg,lw=2,label='Test',c='blue')  \n",
    "    ax.fill_between(batch_sizes_list,test_high,test_low,facecolor='blue',alpha=0.05)\n",
    "    plt.xlabel('Complexity - Batch Size'); plt.ylabel('Mean Square Error'); plt.title('Training and Testing Error vs. Model Complexity (Batch Size)')\n",
    "    combined_error = np.concatenate((train_avg, test_avg), axis=0)\n",
    "    plt.plot([batch_size,batch_size],[max(combined_error)*1.95,min(combined_error)*0.05],c = 'black',linewidth=3,alpha = 0.8)\n",
    "    plt.legend(loc='upper right'); plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1, wspace=0.2, hspace=0.3)\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1f4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: The following code may take a while to run, depending on the parameters selected.\n",
    "# It take minimum of 16 minutes to run on a standard laptop.\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Source: Dr. Michael Pyrcz (GeostatsGuy)\n",
    "# https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Overfit.ipynb\n",
    "# Adapted from: \"Interactive Overfit\" Notebook\n",
    "\n",
    "# Connect the function to the widgets\n",
    "interactive_ann = widgets.interactive_output(\n",
    "    main,\n",
    "    {\n",
    "        'n': n,\n",
    "        'split': split,\n",
    "        'std': std,\n",
    "        'input_nodes': input_nodes,\n",
    "        'input_activation': input_activation,\n",
    "        'num_hidden_layers': num_hidden_layers,\n",
    "        'hidden_nodes': hidden_nodes,\n",
    "        'reduce_nodes': reduce_nodes,\n",
    "        'hidden_activation': hidden_activation,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'optimizer': optimizer,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'epochs': epochs\n",
    "    }\n",
    ")\n",
    "\n",
    "# Optional: reduce flicker\n",
    "interactive_ann.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33166774",
   "metadata": {},
   "source": [
    "## Overfitting Lab Interactive Demonstration\n",
    "[üîù Back to Outline](#outline)\n",
    "\n",
    "> Change the number of sample data, train/test split, data noise, and model structure to observe overfitting!\n",
    "Modify model layers, training parameters, and activations to explore ANN performance.\n",
    "\n",
    "**Data Parameters**\n",
    "- n: Number of data samples\n",
    "- Test %: Percentage of sample data withheld as testing data\n",
    "- Noise StDev: Standard deviation of random Gaussian error added to the data\n",
    "\n",
    "**Input Layer**\n",
    "- input_nodes: Number of input nodes\n",
    "- input_activation: Activation function used in the input layer\n",
    "\n",
    "**Hidden Layers**\n",
    "- num_hidden_layers: Number of hidden layers\n",
    "- hidden_nodes: Number of nodes per hidden layer\n",
    "- reduce_nodes: Option to reduce the number of nodes with depth\n",
    "- hidden_activation: Activation function used in the hidden layers\n",
    "- dropout_rate: Fraction of dropout applied between layers (0 to 0.9)\n",
    "\n",
    "**Training Parameters**\n",
    "- optimizer: Optimizer used to update weights (e.g., adam, sgd, rmsprop)\n",
    "- learning_rate: Step size for the optimizer during backpropagation\n",
    "- batch_size: Number of samples per gradient update\n",
    "- epochs: Number of full training cycles over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46a6f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd7d899adf2456e812129fc74662829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='                                                                                   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929ba3322896483da7989e613811c388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Source: Dr. Michael Pyrcz (GeostatsGuy)\n",
    "# https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Overfit.ipynb\n",
    "# Adapted from: \"Interactive Overfit\" Notebook\n",
    "\n",
    "# It take minimum of 10 minutes to run on a standard laptop.\n",
    "display(ui2, interactive_ann)                           # display the interactive plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f663f",
   "metadata": {},
   "source": [
    "## Observations\n",
    "[üîù Back to Outline](#outline)\n",
    "1. **Model Performance (Top-Left Plot)**\n",
    "The ANN successfully learns the underlying trend in porosity prediction. The training and testing data generally follow the same trend, with minimal overfitting.\n",
    "\n",
    "2. **Loss Curves (Top-Center Plot)**\n",
    "The training and validation loss both decrease smoothly, indicating good convergence. However, the slight gap between them suggests mild overfitting as complexity increases.\n",
    "\n",
    "3. **Error Distribution (Top-Right Plot)**\n",
    "The error distribution for training and testing shows a small spread around the mean, confirming model stability. Yet, testing errors display a longer tail, highlighting some variance in generalization.\n",
    "\n",
    "4. **Hyperparameter Sensitivity (Bottom 6 Plots)**:\n",
    "    - **Epochs**: Error decreases with more epochs, then plateaus, showing that training for too few epochs leads to underfitting, while too many may not add value.\n",
    "    - **Hidden Nodes**: Increased nodes reduce error until diminishing returns or slight overfitting set in beyond ~100 nodes.\n",
    "    - **Hidden Layers**: Adding more layers initially improves performance, but after 3‚Äì4 layers, test error rises, showing increased overfitting.\n",
    "    - **Dropout Rate**: Moderate dropout (e.g., 0.1‚Äì0.2) helps regularize the model. Too much dropout leads to higher error.\n",
    "    - **Learning Rate**: Performance is sensitive to learning rate. Very small or large rates perform poorly. An optimal middle ground exists (~1e-3).\n",
    "    - **Batch Size**: Larger batch sizes lead to higher test error, indicating they may hurt generalization for this dataset.\n",
    "\n",
    "## Conclusion\n",
    "This ANN Lab demonstrates the impact of model complexity and training parameters on overfitting and generalization. While increasing layers, nodes, and epochs can reduce training error, it must be balanced with regularization techniques like dropout and careful tuning of batch size and learning rate.\n",
    "\n",
    "> **Pro Tip: Use sensitivity plots to visually compare different hyperparameter settings. Look for the point where test error is minimized and the gap between training and testing error is smallest ‚Äî this helps you identify the sweet spot between underfitting and overfitting.**\n",
    "\n",
    "In practice, this lab highlights the **importance of iterative tuning and validation** when developing ANN models for geoscientific applications like porosity estimation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<To Do> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793dd431",
   "metadata": {},
   "source": [
    "## About The Author\n",
    "[üîù Back to Outline](#outline)\n",
    "\n",
    "Emmanuel Ikpesu is a first-year PhD student at the [Hildebrand Department of Petroleum and Geosystems Engineering](https://www.pge.utexas.edu/) at [The University of Texas at Austin](https://www.utexas.edu/), within the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p). His research focuses on the integration of **Artificial Intelligence (AI), computational techniques, and information engineering** to address complex challenges in the energy sector, under the guidance of [Professor John T. Foster](https://www.pge.utexas.edu/faculty-and-staff/john-foster/). Currently, Emmanuel is investigating serpentinization processes for **in-situ hydrogen production**, specifically exploring how fractures form in this context, merging AI with physics to uncover novel solutions.\n",
    "\n",
    "Emmanuel brings over **4 years of combined experience** in **data science, data engineering, and reservoir engineering**. He has worked on both upstream oil and gas challenges and large-scale commercial analytics projects.\n",
    "\n",
    "At AB-InBev, he led the optimization and migration of a major data pipeline to Azure, cutting refresh times by 66% and saving the company over 200% in operational costs ‚Äî one of the company‚Äôs standout analytics achievements during his tenure.\n",
    "\n",
    "Outside academia and industry, Emmanuel is passionate about **democratizing data skills**. He shares **free, beginner-friendly content** on **data science** and **data engineering** via [LinkedIn](https://www.linkedin.com/in/emmanuel-ikpesu-393708132/) and [Twitter/X](https://twitter.com/thedatahokage) , aiming to make technical knowledge more accessible to learners around the world.\n",
    "\n",
    "### Open to Collaborations & Internships ü§ù\n",
    "I hope this lab and its resources are helpful to students, educators, and professionals interested in artificial intelligence, data science, and their applications in subsurface modeling and energy systems. I'm always excited to connect, collaborate, and learn.\n",
    "\n",
    "Are you working on innovative problems at the intersection of AI and the energy sector? I‚Äôm open to collaborative research, project-based partnerships, and internship opportunities where I can contribute and grow.\n",
    "\n",
    "If you‚Äôre looking for someone with experience in machine learning, data analytics, and energy modeling‚Äîespecially someone who‚Äôs enthusiastic about combining physics-informed AI with real-world impact‚Äîfeel free to reach out!\n",
    "\n",
    "üì¨ You can contact me at emmanuel.ikpesu@utexas.edu.\n",
    "\n",
    "Let‚Äôs build something meaningful together!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11adb613",
   "metadata": {},
   "source": [
    "## About the Supervising Professor\n",
    "\n",
    "Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p), and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/), at [The University of Texas at Austin](https://www.utexas.edu/), where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,\n",
    "\n",
    "* the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics) freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin\n",
    "\n",
    "* an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board), and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board), the International Association for Mathematical Geosciences. \n",
    "\n",
    "Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en), a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics, co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) and author of two recently released e-books, [Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).\n",
    "\n",
    "All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures) with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy), to support any interested students and working professionals with evergreen content. To find out more about Michael‚Äôs work and shared educational resources visit his [Website](www.michaelpyrcz.com).\n",
    "\n",
    "#### Want to Work Together?\n",
    "\n",
    "I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.\n",
    "\n",
    "* Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I'd be happy to drop by and work with you! \n",
    "\n",
    "* Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!\n",
    "\n",
    "* I can be reached at mpyrcz@austin.utexas.edu.\n",
    "\n",
    "I'm always happy to discuss,\n",
    "\n",
    "*Michael*\n",
    "\n",
    "Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin\n",
    "\n",
    "More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [Applied Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
